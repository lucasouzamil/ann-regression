{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ANN Regression","text":"In\u00a0[5]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns  import tensorflow as tf from tensorflow.keras.utils import plot_model  from sklearn.preprocessing import MinMaxScaler from sklearn.model_selection import train_test_split, cross_val_score from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename)) In\u00a0[56]: Copied! <pre>df = pd.read_csv(\"data/Clean_Dataset.csv\")\ndf = df.drop(['Unnamed: 0'], axis = 1)\nprint(\"\\n================== HEAD DO DATASET ==================\\n\")\ndisplay(df.head())\nprint(\"\\n=====================================================\\n\")\nprint(\"\\n================== SHAPE DO DATASET ==================\\n\")\ndisplay(df.shape)\nprint(\"\\n=====================================================\\n\")\ndisplay(df.dtypes)\nprint(\"\\n================== VISUALIZANDO VALORES NULOS ==================\\n\")\nnull_summary = df.isnull().sum().to_frame().rename(columns={0: 'Null Count'})\nnull_summary['Percentage'] = (null_summary['Null Count'] / len(df)) * 100\ndisplay(null_summary)\nprint(\"\\n=====================================================\\n\")\n</pre> df = pd.read_csv(\"data/Clean_Dataset.csv\") df = df.drop(['Unnamed: 0'], axis = 1) print(\"\\n================== HEAD DO DATASET ==================\\n\") display(df.head()) print(\"\\n=====================================================\\n\") print(\"\\n================== SHAPE DO DATASET ==================\\n\") display(df.shape) print(\"\\n=====================================================\\n\") display(df.dtypes) print(\"\\n================== VISUALIZANDO VALORES NULOS ==================\\n\") null_summary = df.isnull().sum().to_frame().rename(columns={0: 'Null Count'}) null_summary['Percentage'] = (null_summary['Null Count'] / len(df)) * 100 display(null_summary) print(\"\\n=====================================================\\n\") <pre>================== HEAD DO DATASET ==================\n\n</pre> airline flight source_city departure_time stops arrival_time destination_city class duration days_left price 0 SpiceJet SG-8709 Delhi Evening zero Night Mumbai Economy 2.17 1 5953 1 SpiceJet SG-8157 Delhi Early_Morning zero Morning Mumbai Economy 2.33 1 5953 2 AirAsia I5-764 Delhi Early_Morning zero Early_Morning Mumbai Economy 2.17 1 5956 3 Vistara UK-995 Delhi Morning zero Afternoon Mumbai Economy 2.25 1 5955 4 Vistara UK-963 Delhi Morning zero Morning Mumbai Economy 2.33 1 5955 <pre>=====================================================\n\n\n================== SHAPE DO DATASET ==================\n\n</pre> <pre>(300153, 11)</pre> <pre>=====================================================\n\n</pre> <pre>airline              object\nflight               object\nsource_city          object\ndeparture_time       object\nstops                object\narrival_time         object\ndestination_city     object\nclass                object\nduration            float64\ndays_left             int64\nprice                 int64\ndtype: object</pre> <pre>================== VISUALIZANDO VALORES NULOS ==================\n\n</pre> Null Count Percentage airline 0 0.0 flight 0 0.0 source_city 0 0.0 departure_time 0 0.0 stops 0 0.0 arrival_time 0 0.0 destination_city 0 0.0 class 0 0.0 duration 0 0.0 days_left 0 0.0 price 0 0.0 <pre>=====================================================\n\n</pre> <p>Na primeira breve an\u00e1lise, o dataset n\u00e3o possui valores nulos, h\u00e1 v\u00e1rias vari\u00e1veis categ\u00f3ricas que precisam ser tratadas e o target \u00e9 num\u00e9rico cont\u00ednuo.</p> In\u00a0[19]: Copied! <pre># Todas poss\u00edveis vari\u00e1veis categ\u00f3ricas\ncat_features = [column for column in df.columns if df[column].dtype in [\"object\", \"category\", \"bool\"]]\n\n# Num\u00e9ricas mas na verdade s\u00e3o categ\u00f3ricas\nnum_but_cat_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (df[column].nunique() &lt; 9)]\n\n# Vari\u00e1veis categ\u00f3ricas\ncategoric_features = cat_features + num_but_cat_features\ncategoric_features = [column for column in categoric_features]\n\ndf[categoric_features]\n</pre> # Todas poss\u00edveis vari\u00e1veis categ\u00f3ricas cat_features = [column for column in df.columns if df[column].dtype in [\"object\", \"category\", \"bool\"]]  # Num\u00e9ricas mas na verdade s\u00e3o categ\u00f3ricas num_but_cat_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (df[column].nunique() &lt; 9)]  # Vari\u00e1veis categ\u00f3ricas categoric_features = cat_features + num_but_cat_features categoric_features = [column for column in categoric_features]  df[categoric_features] Out[19]: airline flight source_city departure_time stops arrival_time destination_city class 0 SpiceJet SG-8709 Delhi Evening zero Night Mumbai Economy 1 SpiceJet SG-8157 Delhi Early_Morning zero Morning Mumbai Economy 2 AirAsia I5-764 Delhi Early_Morning zero Early_Morning Mumbai Economy 3 Vistara UK-995 Delhi Morning zero Afternoon Mumbai Economy 4 Vistara UK-963 Delhi Morning zero Morning Mumbai Economy ... ... ... ... ... ... ... ... ... 300148 Vistara UK-822 Chennai Morning one Evening Hyderabad Business 300149 Vistara UK-826 Chennai Afternoon one Night Hyderabad Business 300150 Vistara UK-832 Chennai Early_Morning one Night Hyderabad Business 300151 Vistara UK-828 Chennai Early_Morning one Evening Hyderabad Business 300152 Vistara UK-822 Chennai Morning one Evening Hyderabad Business <p>300153 rows \u00d7 8 columns</p> In\u00a0[\u00a0]: Copied! <pre>sns.set_style(\"whitegrid\")\nsns.set_context(\"talk\") \npalette_colors = \"Set2\" \n\nplt.figure(figsize=(20,32))\n\nplt.subplot(4, 2, 1)\nsns.countplot(x=df[\"airline\"], data=df, legend=False)\nplt.title(\"Frequency of Airline\")\nsns.despine() \n\nplt.subplot(4, 2, 2)\nsns.countplot(x=df[\"source_city\"], data=df, palette=palette_colors, \n              hue=df[\"source_city\"], legend=False) \nplt.title(\"Frequency of Source City\")\nsns.despine() \n\nplt.subplot(4, 2, 3)\nsns.countplot(x=df[\"departure_time\"], data=df, palette=palette_colors, \n              hue=df[\"departure_time\"], legend=False)\nplt.title(\"Frequency of Departure Time\")\nsns.despine() \n\nplt.subplot(4, 2, 4)\nsns.countplot(x=df[\"stops\"], data=df, palette=\"deep\", \n              hue=df[\"stops\"], legend=False) \nplt.title(\"Frequency of Stops\")\nsns.despine() \n\nplt.subplot(4, 2, 5)\nsns.countplot(x=df[\"arrival_time\"], data=df, palette=palette_colors, \n              hue=df[\"arrival_time\"], legend=False)\nplt.title(\"Frequency of Arrival Time\")\nsns.despine() \n\nplt.subplot(4, 2, 6)\nsns.countplot(x=df[\"destination_city\"], data=df, palette=palette_colors, \n              hue=df[\"destination_city\"], legend=False)\nplt.title(\"Frequency of Destination City\")\nsns.despine() \n\nplt.subplot(4, 2, 7)\nsns.countplot(x=df[\"class\"], data=df, palette=\"pastel\", \n              hue=df[\"class\"], legend=False) \nplt.title(\"Class Frequency\")\nsns.despine() \n\nplt.tight_layout() \nplt.show()\n</pre> sns.set_style(\"whitegrid\") sns.set_context(\"talk\")  palette_colors = \"Set2\"   plt.figure(figsize=(20,32))  plt.subplot(4, 2, 1) sns.countplot(x=df[\"airline\"], data=df, legend=False) plt.title(\"Frequency of Airline\") sns.despine()   plt.subplot(4, 2, 2) sns.countplot(x=df[\"source_city\"], data=df, palette=palette_colors,                hue=df[\"source_city\"], legend=False)  plt.title(\"Frequency of Source City\") sns.despine()   plt.subplot(4, 2, 3) sns.countplot(x=df[\"departure_time\"], data=df, palette=palette_colors,                hue=df[\"departure_time\"], legend=False) plt.title(\"Frequency of Departure Time\") sns.despine()   plt.subplot(4, 2, 4) sns.countplot(x=df[\"stops\"], data=df, palette=\"deep\",                hue=df[\"stops\"], legend=False)  plt.title(\"Frequency of Stops\") sns.despine()   plt.subplot(4, 2, 5) sns.countplot(x=df[\"arrival_time\"], data=df, palette=palette_colors,                hue=df[\"arrival_time\"], legend=False) plt.title(\"Frequency of Arrival Time\") sns.despine()   plt.subplot(4, 2, 6) sns.countplot(x=df[\"destination_city\"], data=df, palette=palette_colors,                hue=df[\"destination_city\"], legend=False) plt.title(\"Frequency of Destination City\") sns.despine()   plt.subplot(4, 2, 7) sns.countplot(x=df[\"class\"], data=df, palette=\"pastel\",                hue=df[\"class\"], legend=False)  plt.title(\"Class Frequency\") sns.despine()   plt.tight_layout()  plt.show() In\u00a0[20]: Copied! <pre>numeric_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (column not in categoric_features)]\n\ndf[numeric_features]\n</pre> numeric_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (column not in categoric_features)]  df[numeric_features] Out[20]: duration days_left price 0 2.17 1 5953 1 2.33 1 5953 2 2.17 1 5956 3 2.25 1 5955 4 2.33 1 5955 ... ... ... ... 300148 10.08 49 69265 300149 10.42 49 77105 300150 13.83 49 79099 300151 10.00 49 81585 300152 10.08 49 81585 <p>300153 rows \u00d7 3 columns</p> In\u00a0[25]: Copied! <pre>plt.figure(figsize=(20,6))\n\nfor i, col in enumerate([\"duration\", \"days_left\", \"price\"], 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(df[col], kde=True, color=\"skyblue\", bins=30)\n    plt.title(f\"Distribui\u00e7\u00e3o de {col.capitalize()}\", fontsize=14)\n    plt.xlabel(col.capitalize())\n    plt.ylabel(\"Frequ\u00eancia\")\n\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(20,6))  for i, col in enumerate([\"duration\", \"days_left\", \"price\"], 1):     plt.subplot(1, 3, i)     sns.histplot(df[col], kde=True, color=\"skyblue\", bins=30)     plt.title(f\"Distribui\u00e7\u00e3o de {col.capitalize()}\", fontsize=14)     plt.xlabel(col.capitalize())     plt.ylabel(\"Frequ\u00eancia\")  plt.tight_layout() plt.show()  In\u00a0[30]: Copied! <pre>corr = df[\n    [c for c in numeric_features if not c.startswith('Unnamed')]\n].corr()\n\nplt.figure(figsize=(7, 6))\n\ncmap = sns.color_palette(\"vlag\", as_cmap=True) \n\nax = sns.heatmap(\n    corr,\n    cmap=cmap,\n    vmin=-1, vmax=1, center=0,       \n    annot=True, fmt=\".2f\",            \n    annot_kws={\"size\":8},\n    square=True,\n    linewidths=0.4, linecolor=\"white\",\n    cbar_kws={\"shrink\":0.8, \"label\":\"Pearson r\"},\n)\n\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.title(\"Matriz de Correla\u00e7\u00e3o (Pearson)\")\nplt.tight_layout()\nplt.show()\n</pre> corr = df[     [c for c in numeric_features if not c.startswith('Unnamed')] ].corr()  plt.figure(figsize=(7, 6))  cmap = sns.color_palette(\"vlag\", as_cmap=True)   ax = sns.heatmap(     corr,     cmap=cmap,     vmin=-1, vmax=1, center=0,            annot=True, fmt=\".2f\",                 annot_kws={\"size\":8},     square=True,     linewidths=0.4, linecolor=\"white\",     cbar_kws={\"shrink\":0.8, \"label\":\"Pearson r\"}, )  plt.xticks(rotation=45, ha=\"right\") plt.yticks(rotation=0) plt.title(\"Matriz de Correla\u00e7\u00e3o (Pearson)\") plt.tight_layout() plt.show() <p>Uma vez que \"stop\" e \"class\" s\u00e3o vari\u00e1veis orin\u00e1rias, o encoding tamb\u00e9m deve ser de acordo.</p> In\u00a0[31]: Copied! <pre>df[\"stops\"] = df[\"stops\"].replace({'zero': 0,\n                                   'one': 1,\n                                   'two_or_more': 2})\n\ndf[\"class\"] = df[\"class\"].replace({'Economy': 0,\n                                   'Business': 1})\n</pre> df[\"stops\"] = df[\"stops\"].replace({'zero': 0,                                    'one': 1,                                    'two_or_more': 2})  df[\"class\"] = df[\"class\"].replace({'Economy': 0,                                    'Business': 1}) <pre>/tmp/ipykernel_87733/3553766138.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"stops\"] = df[\"stops\"].replace({'zero': 0,\n/tmp/ipykernel_87733/3553766138.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"class\"] = df[\"class\"].replace({'Economy': 0,\n</pre> In\u00a0[\u00a0]: Copied! <pre># One Hot Encoding\ndummies_variables = [\"airline\", \"source_city\", \"departure_time\", \"arrival_time\", \"destination_city\"]\ndummies = pd.get_dummies(df[dummies_variables], drop_first= True)\n\ndf = pd.concat([df, dummies], axis=1)\ndf = df.drop(dummies_variables + ['flight'], axis=1) # Excluindo colunas antigas antes do one hot.\n\ndf.head()\n</pre> # One Hot Encoding dummies_variables = [\"airline\", \"source_city\", \"departure_time\", \"arrival_time\", \"destination_city\"] dummies = pd.get_dummies(df[dummies_variables], drop_first= True)  df = pd.concat([df, dummies], axis=1) df = df.drop(dummies_variables + ['flight'], axis=1) # Excluindo colunas antigas antes do one hot.  df.head() Out[\u00a0]: stops class duration days_left price airline_Air_India airline_GO_FIRST airline_Indigo airline_SpiceJet airline_Vistara ... arrival_time_Early_Morning arrival_time_Evening arrival_time_Late_Night arrival_time_Morning arrival_time_Night destination_city_Chennai destination_city_Delhi destination_city_Hyderabad destination_city_Kolkata destination_city_Mumbai 0 0 0 2.17 1 5953 False False False True False ... False False False False True False False False False True 1 0 0 2.33 1 5953 False False False True False ... False False False True False False False False False True 2 0 0 2.17 1 5956 False False False False False ... True False False False False False False False False True 3 0 0 2.25 1 5955 False False False False True ... False False False False False False False False False True 4 0 0 2.33 1 5955 False False False False True ... False False False True False False False False False True <p>5 rows \u00d7 30 columns</p> In\u00a0[33]: Copied! <pre>df.shape\n</pre> df.shape Out[33]: <pre>(300153, 30)</pre> <p>Ap\u00f3s o one hot, 18 novas featutes surgiram.</p> In\u00a0[34]: Copied! <pre>corr = df.corr(numeric_only=True)\nplt.figure(figsize=(35, 25))\nax = sns.heatmap(\n    corr,                      \n    annot=True, fmt=\".2f\",\n    vmin=-1.0, vmax=1.0, center=0,\n    cmap=\"vlag\",             \n    square=True,\n    linewidths=0.4, linecolor=\"white\",\n    cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"},\n)\n\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.title(\"Matriz de Correla\u00e7\u00e3o\")\nplt.tight_layout()\nplt.show()\n</pre> corr = df.corr(numeric_only=True) plt.figure(figsize=(35, 25)) ax = sns.heatmap(     corr,                           annot=True, fmt=\".2f\",     vmin=-1.0, vmax=1.0, center=0,     cmap=\"vlag\",                  square=True,     linewidths=0.4, linecolor=\"white\",     cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"}, )  plt.xticks(rotation=45, ha=\"right\") plt.yticks(rotation=0) plt.title(\"Matriz de Correla\u00e7\u00e3o\") plt.tight_layout() plt.show() In\u00a0[37]: Copied! <pre>import numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nSEED = 42\n\ny = df['price']\nX = df.drop('price', axis=1)\n</pre> import numpy as np, pandas as pd from sklearn.model_selection import train_test_split SEED = 42  y = df['price'] X = df.drop('price', axis=1) In\u00a0[38]: Copied! <pre>X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.30, random_state=SEED)\nX_val,   X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=SEED)\n\nprint(f\"Train ({(len(X_train)/len(X)):.2f}): {len(X_train)} | Val ({len(X_val)/len(X):.2f}): {len(X_val)} | Test ({len(X_test)/len(X):.2f}): {len(X_test)}\")\n</pre> X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.30, random_state=SEED) X_val,   X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=SEED)  print(f\"Train ({(len(X_train)/len(X)):.2f}): {len(X_train)} | Val ({len(X_val)/len(X):.2f}): {len(X_val)} | Test ({len(X_test)/len(X):.2f}): {len(X_test)}\") <pre>Train (0.70): 210107 | Val (0.15): 45023 | Test (0.15): 45023\n</pre> In\u00a0[39]: Copied! <pre>X_train\n</pre> X_train Out[39]: stops class duration days_left airline_Air_India airline_GO_FIRST airline_Indigo airline_SpiceJet airline_Vistara source_city_Chennai ... arrival_time_Early_Morning arrival_time_Evening arrival_time_Late_Night arrival_time_Morning arrival_time_Night destination_city_Chennai destination_city_Delhi destination_city_Hyderabad destination_city_Kolkata destination_city_Mumbai 2406 1 0 13.42 14 False False False False True False ... False False False True False False False False False True 275865 1 1 9.58 23 False False False False True False ... False True False False False False True False False False 297156 1 1 11.17 29 False False False False True True ... False True False False False False False False True False 12826 1 0 5.08 16 False False False False True False ... False False False False True False False False False False 93166 1 0 12.58 45 False True False False False False ... False False False False True False True False False False ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 119879 1 0 20.50 2 False False False False True False ... False False False False False False True False False False 259178 1 1 25.42 7 False False False False True False ... False False False False True True False False False False 131932 1 0 13.67 29 True False False False False False ... False False False True False False False False False True 146867 1 0 8.33 39 False True False False False False ... False False False False True False False True False False 121958 1 0 20.17 17 True False False False False False ... False False False True False False True False False False <p>210107 rows \u00d7 29 columns</p> In\u00a0[40]: Copied! <pre># Scaling AP\u00d3S o split (sem tocar na vari\u00e1vel-alvo)\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnum_vars = ['duration', 'days_left']  # \u2b05\ufe0f n\u00e3o inclui 'price'\n\n# Ajusta no treino e aplica em val/test (sem vazamento)\nX_train.loc[:, num_vars] = scaler.fit_transform(X_train[num_vars])\nX_val.loc[:,   num_vars] = scaler.transform(X_val[num_vars])\nX_test.loc[:,  num_vars] = scaler.transform(X_test[num_vars])\n</pre> # Scaling AP\u00d3S o split (sem tocar na vari\u00e1vel-alvo) from sklearn.preprocessing import MinMaxScaler  scaler = MinMaxScaler() num_vars = ['duration', 'days_left']  # \u2b05\ufe0f n\u00e3o inclui 'price'  # Ajusta no treino e aplica em val/test (sem vazamento) X_train.loc[:, num_vars] = scaler.fit_transform(X_train[num_vars]) X_val.loc[:,   num_vars] = scaler.transform(X_val[num_vars]) X_test.loc[:,  num_vars] = scaler.transform(X_test[num_vars]) <pre>/tmp/ipykernel_87733/2227656102.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.27083333 0.45833333 0.58333333 ... 0.58333333 0.79166667 0.33333333]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X_train.loc[:, num_vars] = scaler.fit_transform(X_train[num_vars])\n/tmp/ipykernel_87733/2227656102.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.20833333 0.875      0.27083333 ... 1.         1.         0.47916667]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X_val.loc[:,   num_vars] = scaler.transform(X_val[num_vars])\n/tmp/ipykernel_87733/2227656102.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.0625     0.5625     0.77083333 ... 0.25       0.10416667 0.64583333]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X_test.loc[:,  num_vars] = scaler.transform(X_test[num_vars])\n</pre> <p>A rede neural artificial (ANN) implementada realiza uma tarefa de regress\u00e3o supervisionada, com o objetivo de prever um valor cont\u00ednuo (no caso, o pre\u00e7o da passagem a\u00e9rea) a partir de 30 vari\u00e1veis de entrada previamente processadas.</p> <p>O modelo foi constru\u00eddo do zero, utilizando apenas NumPy, sem o uso de frameworks de alto n\u00edvel como TensorFlow ou PyTorch, o que permite controle total sobre o fluxo de c\u00e1lculo e o aprendizado.</p> <p>A arquitetura definida segue o padr\u00e3o feedforward totalmente conectado (Multilayer Perceptron \u2013 MLP), composta por:</p> <ul> <li>Camada de entrada: 30 neur\u00f4nios (uma para cada feature num\u00e9rica e categ\u00f3rica codificada).</li> <li>1\u00aa camada oculta: 32 neur\u00f4nios com fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU (Rectified Linear Unit).</li> <li>2\u00aa camada oculta: 16 neur\u00f4nios, tamb\u00e9m com ativa\u00e7\u00e3o ReLU.</li> <li>Camada de sa\u00edda: 1 neur\u00f4nio com sa\u00edda linear, adequada para tarefas de regress\u00e3o (onde o valor previsto \u00e9 cont\u00ednuo).</li> </ul> In\u00a0[54]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom math import sqrt\n\nX_train_base = X_train   \nX_val_base = X_val   \nX_test_base = X_test\n\nX_train = np.asarray(X_train_base, dtype=float)\nX_val   = np.asarray(X_val_base,   dtype=float)\nX_test  = np.asarray(X_test_base,  dtype=float)\n\ny_train = np.asarray(y_train, dtype=float).ravel()\ny_val   = np.asarray(y_val,   dtype=float).ravel()\ny_test  = np.asarray(y_test,  dtype=float).ravel()\n\ny_train_col = y_train.reshape(-1, 1)\ny_val_col   = y_val.reshape(-1, 1)\n\nN, D = X_train.shape\n\n# 1) M\u00e9tricas de regress\u00e3o\ndef metrics(y_true, y_pred):\n    y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()\n    mae  = np.mean(np.abs(y_true - y_pred))\n    rmse = sqrt(np.mean((y_true - y_pred)**2))\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n    r2 = 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan\n    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n\n# 2) Arquitetura: ReLU 128\u219264\u21921 (linear)\nrng = np.random.default_rng(42)\nH1, H2 = 32, 16\n\nW1 = rng.standard_normal((H1, D)) * np.sqrt(2.0 / D)\nb1 = np.zeros((1, H1))\n\nW2 = rng.standard_normal((H2, H1)) * np.sqrt(2.0 / H1) \nb2 = np.zeros((1, H2))\n\nW3 = rng.standard_normal((1, H2)) * np.sqrt(2.0 / H2)\nb3 = np.zeros((1, 1))\n\ndef relu(x):    \n    return np.maximum(0, x)\n\ndef relu_d(x):  \n    return (x &gt; 0).astype(x.dtype)\n\ndef forward(X):\n    Z1 = X @ W1.T + b1; A1 = relu(Z1)\n    Z2 = A1 @ W2.T + b2; A2 = relu(Z2)\n    Y  = A2 @ W3.T + b3          # sa\u00edda linear (regress\u00e3o)\n    return Z1, A1, Z2, A2, Y\n\n# 3) Treino: Adam + MSE\nlr = 1e-3\nepochs = 100\nbatch  = 128\n\nbeta1, beta2, eps = 0.9, 0.999, 1e-8\n\n# Adam buffers\nmW1 = np.zeros_like(W1) \nvW1 = np.zeros_like(W1)\n\nmW2 = np.zeros_like(W2)\nvW2 = np.zeros_like(W2)\n\nmW3 = np.zeros_like(W3)\nvW3 = np.zeros_like(W3)\n\nmb1 = np.zeros_like(b1)\nvb1 = np.zeros_like(b1)\n\nmb2 = np.zeros_like(b2) \nvb2 = np.zeros_like(b2)\n\nmb3 = np.zeros_like(b3)\nvb3 = np.zeros_like(b3)\n\nt = 0\n\ndef adam_update(param, grad, m, v):\n    m = beta1*m + (1-beta1)*grad\n    v = beta2*v + (1-beta2)*(grad*grad)\n    m_hat = m / (1 - beta1**t)\n    v_hat = v / (1 - beta2**t)\n    param -= lr * m_hat / (np.sqrt(v_hat) + eps)\n    return param, m, v\n\ntrain_losses, val_losses = [], []\n\nfor ep in range(1, epochs+1):\n    idx = rng.permutation(N)\n    epoch_loss = 0.0\n\n    for i in range(0, N, batch):\n        bidx = idx[i:i+batch]\n        xb = X_train[bidx]\n        yb = y_train_col[bidx]\n        B  = xb.shape[0]\n\n        #  forward \n        Z1, A1, Z2, A2, Yp = forward(xb)\n\n        #  loss (MSE)\n        mse = np.mean((Yp - yb)**2)\n        epoch_loss += mse * B\n\n        #  backprop \n        d3  = (2.0/B) * (Yp - yb)     # (B,1)\n        gW3 = d3.T @ A2               # (1,H2)\n        gb3 = d3.sum(axis=0, keepdims=True)\n\n        dA2 = d3 @ W3                 # (B,H2)\n        dZ2 = dA2 * relu_d(Z2)\n        gW2 = dZ2.T @ A1              # (H2,H1)\n        gb2 = dZ2.sum(axis=0, keepdims=True)\n\n        dA1 = dZ2 @ W2                # (B,H1)\n        dZ1 = dA1 * relu_d(Z1)\n        gW1 = dZ1.T @ xb              # (H1,D)\n        gb1 = dZ1.sum(axis=0, keepdims=True)\n\n        #  Adam step (incrementa t uma vez por batch) \n        t += 1\n        W3, mW3, vW3 = adam_update(W3, gW3, mW3, vW3)\n        b3, mb3, vb3 = adam_update(b3, gb3, mb3, vb3)\n        W2, mW2, vW2 = adam_update(W2, gW2, mW2, vW2)\n        b2, mb2, vb2 = adam_update(b2, gb2, mb2, vb2)\n        W1, mW1, vW1 = adam_update(W1, gW1, mW1, vW1)\n        b1, mb1, vb1 = adam_update(b1, gb1, mb1, vb1)\n\n    epoch_loss /= N\n    train_losses.append(epoch_loss)\n\n    # valida\u00e7\u00e3o\n    _, _, _, _, Yv = forward(X_val)\n    val_mse = np.mean((Yv - y_val_col)**2)\n    val_losses.append(val_mse)\n\n    print(f\"\u00c9poca {ep:02d} | Train MSE {epoch_loss:.4f} | Val MSE {val_mse:.4f}\")\n\n# 4) Avalia\u00e7\u00e3o e gr\u00e1ficos\ndef predict(X):\n    _, _, _, _, Y = forward(X)\n    return Y.ravel()\n\npred_val  = predict(X_val)\npred_test = predict(X_test)\n\nprint(\"Val :\", metrics(y_val,  pred_val))\nprint(\"Test:\", metrics(y_test, pred_test))\n\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(train_losses, label=\"Train\")\nplt.plot(val_losses,   label=\"Val\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\"); plt.title(\"Curva de Loss\"); plt.legend()\n\nplt.subplot(1,2,2)\nplt.scatter(y_val, pred_val, s=8, alpha=0.4)\nlims = [min(y_val.min(), pred_val.min()), max(y_val.max(), pred_val.max())]\nplt.plot(lims, lims, \"--\")\nplt.xlabel(\"Verdadeiro\"); plt.ylabel(\"Previsto\"); plt.title(\"Val: y_true vs y_pred\")\nplt.tight_layout(); plt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from math import sqrt  X_train_base = X_train    X_val_base = X_val    X_test_base = X_test  X_train = np.asarray(X_train_base, dtype=float) X_val   = np.asarray(X_val_base,   dtype=float) X_test  = np.asarray(X_test_base,  dtype=float)  y_train = np.asarray(y_train, dtype=float).ravel() y_val   = np.asarray(y_val,   dtype=float).ravel() y_test  = np.asarray(y_test,  dtype=float).ravel()  y_train_col = y_train.reshape(-1, 1) y_val_col   = y_val.reshape(-1, 1)  N, D = X_train.shape  # 1) M\u00e9tricas de regress\u00e3o def metrics(y_true, y_pred):     y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()     mae  = np.mean(np.abs(y_true - y_pred))     rmse = sqrt(np.mean((y_true - y_pred)**2))     ss_res = np.sum((y_true - y_pred)**2)     ss_tot = np.sum((y_true - np.mean(y_true))**2)     r2 = 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan     return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}  # 2) Arquitetura: ReLU 128\u219264\u21921 (linear) rng = np.random.default_rng(42) H1, H2 = 32, 16  W1 = rng.standard_normal((H1, D)) * np.sqrt(2.0 / D) b1 = np.zeros((1, H1))  W2 = rng.standard_normal((H2, H1)) * np.sqrt(2.0 / H1)  b2 = np.zeros((1, H2))  W3 = rng.standard_normal((1, H2)) * np.sqrt(2.0 / H2) b3 = np.zeros((1, 1))  def relu(x):         return np.maximum(0, x)  def relu_d(x):       return (x &gt; 0).astype(x.dtype)  def forward(X):     Z1 = X @ W1.T + b1; A1 = relu(Z1)     Z2 = A1 @ W2.T + b2; A2 = relu(Z2)     Y  = A2 @ W3.T + b3          # sa\u00edda linear (regress\u00e3o)     return Z1, A1, Z2, A2, Y  # 3) Treino: Adam + MSE lr = 1e-3 epochs = 100 batch  = 128  beta1, beta2, eps = 0.9, 0.999, 1e-8  # Adam buffers mW1 = np.zeros_like(W1)  vW1 = np.zeros_like(W1)  mW2 = np.zeros_like(W2) vW2 = np.zeros_like(W2)  mW3 = np.zeros_like(W3) vW3 = np.zeros_like(W3)  mb1 = np.zeros_like(b1) vb1 = np.zeros_like(b1)  mb2 = np.zeros_like(b2)  vb2 = np.zeros_like(b2)  mb3 = np.zeros_like(b3) vb3 = np.zeros_like(b3)  t = 0  def adam_update(param, grad, m, v):     m = beta1*m + (1-beta1)*grad     v = beta2*v + (1-beta2)*(grad*grad)     m_hat = m / (1 - beta1**t)     v_hat = v / (1 - beta2**t)     param -= lr * m_hat / (np.sqrt(v_hat) + eps)     return param, m, v  train_losses, val_losses = [], []  for ep in range(1, epochs+1):     idx = rng.permutation(N)     epoch_loss = 0.0      for i in range(0, N, batch):         bidx = idx[i:i+batch]         xb = X_train[bidx]         yb = y_train_col[bidx]         B  = xb.shape[0]          #  forward          Z1, A1, Z2, A2, Yp = forward(xb)          #  loss (MSE)         mse = np.mean((Yp - yb)**2)         epoch_loss += mse * B          #  backprop          d3  = (2.0/B) * (Yp - yb)     # (B,1)         gW3 = d3.T @ A2               # (1,H2)         gb3 = d3.sum(axis=0, keepdims=True)          dA2 = d3 @ W3                 # (B,H2)         dZ2 = dA2 * relu_d(Z2)         gW2 = dZ2.T @ A1              # (H2,H1)         gb2 = dZ2.sum(axis=0, keepdims=True)          dA1 = dZ2 @ W2                # (B,H1)         dZ1 = dA1 * relu_d(Z1)         gW1 = dZ1.T @ xb              # (H1,D)         gb1 = dZ1.sum(axis=0, keepdims=True)          #  Adam step (incrementa t uma vez por batch)          t += 1         W3, mW3, vW3 = adam_update(W3, gW3, mW3, vW3)         b3, mb3, vb3 = adam_update(b3, gb3, mb3, vb3)         W2, mW2, vW2 = adam_update(W2, gW2, mW2, vW2)         b2, mb2, vb2 = adam_update(b2, gb2, mb2, vb2)         W1, mW1, vW1 = adam_update(W1, gW1, mW1, vW1)         b1, mb1, vb1 = adam_update(b1, gb1, mb1, vb1)      epoch_loss /= N     train_losses.append(epoch_loss)      # valida\u00e7\u00e3o     _, _, _, _, Yv = forward(X_val)     val_mse = np.mean((Yv - y_val_col)**2)     val_losses.append(val_mse)      print(f\"\u00c9poca {ep:02d} | Train MSE {epoch_loss:.4f} | Val MSE {val_mse:.4f}\")  # 4) Avalia\u00e7\u00e3o e gr\u00e1ficos def predict(X):     _, _, _, _, Y = forward(X)     return Y.ravel()  pred_val  = predict(X_val) pred_test = predict(X_test)  print(\"Val :\", metrics(y_val,  pred_val)) print(\"Test:\", metrics(y_test, pred_test))  plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.plot(train_losses, label=\"Train\") plt.plot(val_losses,   label=\"Val\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\"); plt.title(\"Curva de Loss\"); plt.legend()  plt.subplot(1,2,2) plt.scatter(y_val, pred_val, s=8, alpha=0.4) lims = [min(y_val.min(), pred_val.min()), max(y_val.max(), pred_val.max())] plt.plot(lims, lims, \"--\") plt.xlabel(\"Verdadeiro\"); plt.ylabel(\"Previsto\"); plt.title(\"Val: y_true vs y_pred\") plt.tight_layout(); plt.show() <pre>\u00c9poca 01 | Train MSE 678509857.4816 | Val MSE 398164542.4253\n\u00c9poca 02 | Train MSE 326570275.8006 | Val MSE 248977535.9151\n\u00c9poca 03 | Train MSE 167966263.4366 | Val MSE 95650901.6718\n\u00c9poca 04 | Train MSE 62077396.4720 | Val MSE 47291686.2173\n\u00c9poca 05 | Train MSE 42988600.4069 | Val MSE 42002038.3514\n\u00c9poca 06 | Train MSE 39708288.1543 | Val MSE 39712292.8742\n\u00c9poca 07 | Train MSE 37961861.7792 | Val MSE 38414171.9839\n\u00c9poca 08 | Train MSE 36873997.5994 | Val MSE 37462698.1271\n\u00c9poca 09 | Train MSE 36046750.1699 | Val MSE 36682944.0764\n\u00c9poca 10 | Train MSE 35317419.2662 | Val MSE 35993500.8448\n\u00c9poca 11 | Train MSE 34632413.1965 | Val MSE 35377141.7630\n\u00c9poca 12 | Train MSE 34014803.2772 | Val MSE 34735166.3910\n\u00c9poca 13 | Train MSE 33459533.5891 | Val MSE 34206057.7083\n\u00c9poca 14 | Train MSE 32954468.3794 | Val MSE 33734255.0602\n\u00c9poca 15 | Train MSE 32493218.0879 | Val MSE 33285006.0355\n\u00c9poca 16 | Train MSE 32073442.7710 | Val MSE 32837303.2367\n\u00c9poca 17 | Train MSE 31692858.1649 | Val MSE 32475898.6631\n\u00c9poca 18 | Train MSE 31347365.0347 | Val MSE 32104119.8431\n\u00c9poca 19 | Train MSE 31028605.0552 | Val MSE 31797864.3642\n\u00c9poca 20 | Train MSE 30761997.4167 | Val MSE 31528169.5052\n\u00c9poca 21 | Train MSE 30514767.7398 | Val MSE 31282686.8446\n\u00c9poca 22 | Train MSE 30295995.6337 | Val MSE 31077958.8826\n\u00c9poca 23 | Train MSE 30112897.3051 | Val MSE 30914770.8588\n\u00c9poca 24 | Train MSE 29949203.5508 | Val MSE 30743507.9177\n\u00c9poca 25 | Train MSE 29802686.8104 | Val MSE 30548239.1552\n\u00c9poca 26 | Train MSE 29661708.6134 | Val MSE 30393361.4386\n\u00c9poca 27 | Train MSE 29535435.6979 | Val MSE 30309425.7199\n\u00c9poca 28 | Train MSE 29417528.0130 | Val MSE 30166478.4950\n\u00c9poca 29 | Train MSE 29299797.7066 | Val MSE 30040308.5257\n\u00c9poca 30 | Train MSE 29200205.5227 | Val MSE 29921136.0605\n\u00c9poca 31 | Train MSE 29111662.9087 | Val MSE 29811439.1397\n\u00c9poca 32 | Train MSE 29003781.6697 | Val MSE 29766148.0476\n\u00c9poca 33 | Train MSE 28918996.7807 | Val MSE 29635477.4848\n\u00c9poca 34 | Train MSE 28827376.0748 | Val MSE 29520364.5602\n\u00c9poca 35 | Train MSE 28726392.2459 | Val MSE 29415309.2885\n\u00c9poca 36 | Train MSE 28626951.2461 | Val MSE 29322534.5992\n\u00c9poca 37 | Train MSE 28526149.2728 | Val MSE 29232215.9211\n\u00c9poca 38 | Train MSE 28406294.0774 | Val MSE 29120707.5696\n\u00c9poca 39 | Train MSE 28267459.0380 | Val MSE 28930305.1212\n\u00c9poca 40 | Train MSE 28096645.9363 | Val MSE 28755814.9254\n\u00c9poca 41 | Train MSE 27872789.8970 | Val MSE 28505155.5965\n\u00c9poca 42 | Train MSE 27645605.1708 | Val MSE 28317125.3604\n\u00c9poca 43 | Train MSE 27435152.8005 | Val MSE 28058055.6164\n\u00c9poca 44 | Train MSE 27210915.4433 | Val MSE 27826197.8575\n\u00c9poca 45 | Train MSE 26984133.8583 | Val MSE 27610014.3938\n\u00c9poca 46 | Train MSE 26746877.4561 | Val MSE 27391632.2307\n\u00c9poca 47 | Train MSE 26505597.2339 | Val MSE 27138844.3174\n\u00c9poca 48 | Train MSE 26260932.3331 | Val MSE 26874781.5938\n\u00c9poca 49 | Train MSE 25980388.0590 | Val MSE 26596627.9164\n\u00c9poca 50 | Train MSE 25732835.4208 | Val MSE 26380494.2580\n\u00c9poca 51 | Train MSE 25490131.6671 | Val MSE 26261886.9336\n\u00c9poca 52 | Train MSE 25283257.2011 | Val MSE 25974609.5864\n\u00c9poca 53 | Train MSE 25083967.0691 | Val MSE 25781837.3601\n\u00c9poca 54 | Train MSE 24887912.7925 | Val MSE 25893364.7678\n\u00c9poca 55 | Train MSE 24725293.9297 | Val MSE 25417175.5198\n\u00c9poca 56 | Train MSE 24559105.0121 | Val MSE 25283222.6262\n\u00c9poca 57 | Train MSE 24410687.5923 | Val MSE 25132808.3956\n\u00c9poca 58 | Train MSE 24256907.3456 | Val MSE 25098362.9468\n\u00c9poca 59 | Train MSE 24104936.0335 | Val MSE 24815227.1668\n\u00c9poca 60 | Train MSE 23962774.6408 | Val MSE 24700838.6018\n\u00c9poca 61 | Train MSE 23818127.3866 | Val MSE 24525977.2965\n\u00c9poca 62 | Train MSE 23688804.0406 | Val MSE 24414001.4563\n\u00c9poca 63 | Train MSE 23559978.4693 | Val MSE 24265398.2340\n\u00c9poca 64 | Train MSE 23443781.1897 | Val MSE 24157470.8105\n\u00c9poca 65 | Train MSE 23330519.8728 | Val MSE 24084077.5306\n\u00c9poca 66 | Train MSE 23224405.8256 | Val MSE 23958684.3995\n\u00c9poca 67 | Train MSE 23117822.4916 | Val MSE 23851343.9213\n\u00c9poca 68 | Train MSE 23032522.0461 | Val MSE 23773041.7690\n\u00c9poca 69 | Train MSE 22945403.2833 | Val MSE 23673327.5016\n\u00c9poca 70 | Train MSE 22862552.0517 | Val MSE 23580895.8501\n\u00c9poca 71 | Train MSE 22788407.7972 | Val MSE 23508502.2706\n\u00c9poca 72 | Train MSE 22716801.7800 | Val MSE 23448826.7116\n\u00c9poca 73 | Train MSE 22651504.6391 | Val MSE 23374096.7541\n\u00c9poca 74 | Train MSE 22543559.8271 | Val MSE 23260535.9626\n\u00c9poca 75 | Train MSE 22429042.1169 | Val MSE 23150273.1690\n\u00c9poca 76 | Train MSE 22327322.6257 | Val MSE 23034294.4389\n\u00c9poca 77 | Train MSE 22228108.5185 | Val MSE 22934799.4622\n\u00c9poca 78 | Train MSE 22140555.7481 | Val MSE 22883420.4062\n\u00c9poca 79 | Train MSE 22062846.1529 | Val MSE 22800066.4794\n\u00c9poca 80 | Train MSE 21978606.1706 | Val MSE 22687620.6776\n\u00c9poca 81 | Train MSE 21894684.1021 | Val MSE 22642557.3078\n\u00c9poca 82 | Train MSE 21820379.9311 | Val MSE 22552175.5755\n\u00c9poca 83 | Train MSE 21756795.1734 | Val MSE 22576422.2681\n\u00c9poca 84 | Train MSE 21688711.3699 | Val MSE 22432106.0638\n\u00c9poca 85 | Train MSE 21633667.4533 | Val MSE 22344762.2909\n\u00c9poca 86 | Train MSE 21584896.0175 | Val MSE 22404937.2086\n\u00c9poca 87 | Train MSE 21529884.7170 | Val MSE 22264439.5486\n\u00c9poca 88 | Train MSE 21469470.9937 | Val MSE 22222362.1503\n\u00c9poca 89 | Train MSE 21410579.3042 | Val MSE 22165393.1789\n\u00c9poca 90 | Train MSE 21354969.2609 | Val MSE 22095142.5159\n\u00c9poca 91 | Train MSE 21301860.7765 | Val MSE 22090995.9187\n\u00c9poca 92 | Train MSE 21259211.6110 | Val MSE 21968464.5830\n\u00c9poca 93 | Train MSE 21201740.6738 | Val MSE 21941721.5422\n\u00c9poca 94 | Train MSE 21162246.5955 | Val MSE 21897818.8100\n\u00c9poca 95 | Train MSE 21123956.3953 | Val MSE 21840419.5727\n\u00c9poca 96 | Train MSE 21081397.3961 | Val MSE 21812064.0865\n\u00c9poca 97 | Train MSE 21042992.2923 | Val MSE 21763704.7335\n\u00c9poca 98 | Train MSE 21012378.9608 | Val MSE 21809897.7625\n\u00c9poca 99 | Train MSE 20981444.4583 | Val MSE 21741157.0445\n\u00c9poca 100 | Train MSE 20943550.3786 | Val MSE 21706152.6864\nVal : {'MAE': np.float64(2719.284140828667), 'RMSE': 4658.986229469688, 'R2': np.float64(0.9581852820771005)}\nTest: {'MAE': np.float64(2702.177655808821), 'RMSE': 4554.172782841557, 'R2': np.float64(0.959349325120191)}\n</pre> In\u00a0[55]: Copied! <pre>from math import sqrt\nimport matplotlib.pyplot as plt\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 12,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n    \"figure.titlesize\": 13\n})\n\n# Fun\u00e7\u00f5es auxiliares de m\u00e9tricas\ndef _to1d(a):\n    a = np.asarray(a, dtype=float)\n    return a.ravel() if a.ndim &gt; 1 else a\n\ndef mape(y_true, y_pred, eps=1e-8):\n    y_true = np.asarray(y_true, dtype=float)\n    denom = np.maximum(np.abs(y_true), eps)\n    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0\n\ndef smape(y_true, y_pred, eps=1e-8):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    denom = np.maximum((np.abs(y_true) + np.abs(y_pred)) / 2, eps)\n    return np.mean(np.abs(y_true - y_pred) / denom) * 100.0\n\ndef r2_score_np(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=float)\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n    return 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan\n\n# Dados e previs\u00f5es\nX_val   = np.asarray(X_val, dtype=float)\nX_test  = np.asarray(X_test, dtype=float)\ny_train = _to1d(y_train)\ny_val   = _to1d(y_val)\ny_test  = _to1d(y_test)\n\ny_pred_val  = _to1d(predict(X_val))\ny_pred_test = _to1d(predict(X_test))\n\nbaseline_val  = np.full_like(y_val,  fill_value=float(np.mean(y_train)), dtype=float)\nbaseline_test = np.full_like(y_test, fill_value=float(np.mean(y_train)), dtype=float)\n\n# Fun\u00e7\u00e3o de m\u00e9tricas gerais\ndef print_metrics(title, y_true, y_pred, y_base):\n    mae  = np.mean(np.abs(y_true - y_pred))\n    mse  = np.mean((y_true - y_pred)**2)\n    rmse = sqrt(mse)\n    r2   = r2_score_np(y_true, y_pred)\n    mp   = mape(y_true, y_pred)\n    smp  = smape(y_true, y_pred)\n\n    mae_b  = np.mean(np.abs(y_true - y_base))\n    mse_b  = np.mean((y_true - y_base)**2)\n    rmse_b = sqrt(mse_b)\n    r2_b   = r2_score_np(y_true, y_base)\n    mp_b   = mape(y_true, y_base)\n    smp_b  = smape(y_true, y_base)\n\n    print(f\"\\n=== {title} ===\")\n    print(\"Metric           Model         Baseline(mean y_train)\")\n    print(f\"MAE      :   {mae:10.4f}   {mae_b:10.4f}\")\n    print(f\"MSE      :   {mse:10.4f}   {mse_b:10.4f}\")\n    print(f\"RMSE     :   {rmse:10.4f}   {rmse_b:10.4f}\")\n    print(f\"R2       :   {r2:10.4f}   {r2_b:10.4f}\")\n    print(f\"MAPE(%)  :   {mp:10.4f}   {mp_b:10.4f}\")\n    print(f\"sMAPE(%) :   {smp:10.4f}   {smp_b:10.4f}\")\n\nprint_metrics(\"Validation\", y_val,  y_pred_val,  baseline_val)\nprint_metrics(\"Test\",       y_test, y_pred_test, baseline_test)\n\n# ------------------------------------------------------------\ntrain_acc = 1 - np.array(train_losses) / np.max(train_losses)\nval_acc   = 1 - np.array(val_losses) / np.max(val_losses)\n\nss_tot_val = np.sum((y_val - np.mean(y_val))**2)\nr2_by_epoch = [1 - (mse * len(y_val)) / ss_tot_val for mse in val_losses]\n\nfinal_mse_test = np.mean((y_pred_test - y_test)**2)\ntest_acc = 1 - final_mse_test / np.max(train_losses)\n\nplt.figure(figsize=(15,4))\n\nplt.subplot(1,3,1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\")\nplt.title(\"Curva de Loss por \u00c9poca\")\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(train_acc, label=\"Train Accuracy (1 - Loss Normalizada)\")\nplt.plot(val_acc, label=\"Val Accuracy (1 - Loss Normalizada)\")\nplt.hlines(test_acc, 0, len(train_acc)-1, colors='orange', linestyles='--', label=f\"Test Accuracy ({test_acc:.3f})\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"Acur\u00e1cia (Normalizada)\")\nplt.title(\"Acur\u00e1cia por \u00c9poca (Train / Val / Test)\")\nplt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(r2_by_epoch, color='green', label=\"R\u00b2 (Val)\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"R\u00b2\")\nplt.title(\"R\u00b2 por \u00c9poca (Valida\u00e7\u00e3o)\")\nplt.ylim(0, 1.05)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 2) Gr\u00e1ficos de res\u00edduos e previs\u00e3o x real\nres_val = y_val - y_pred_val\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.scatter(y_pred_val, res_val, alpha=0.6)\nplt.axhline(0, linestyle=\"--\")\nplt.xlabel(\"Predicted (val)\"); plt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot (Val)\")\n\nplt.subplot(1,2,2)\nplt.scatter(y_val, y_pred_val, alpha=0.6)\nmn = float(min(np.min(y_val), np.min(y_pred_val)))\nmx = float(max(np.max(y_val), np.max(y_pred_val)))\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(\"Actual (y_val)\"); plt.ylabel(\"Predicted (val)\")\nplt.title(\"Actual vs Predicted (Val)\")\nplt.tight_layout()\nplt.show()\n\nres_test = y_test - y_pred_test\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.scatter(y_pred_test, res_test, alpha=0.6)\nplt.axhline(0, linestyle=\"--\")\nplt.xlabel(\"Predicted (test)\"); plt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot (Test)\")\n\nplt.subplot(1,2,2)\nplt.scatter(y_test, y_pred_test, alpha=0.6)\nmn = float(min(np.min(y_test), np.min(y_pred_test)))\nmx = float(max(np.max(y_test), np.max(y_pred_test)))\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(\"Actual (y_test)\"); plt.ylabel(\"Predicted (test)\")\nplt.title(\"Actual vs Predicted (Test)\")\nplt.tight_layout()\nplt.show()\n</pre> from math import sqrt import matplotlib.pyplot as plt plt.rcParams.update({     \"font.size\": 10,     \"axes.titlesize\": 12,     \"axes.labelsize\": 10,     \"xtick.labelsize\": 9,     \"ytick.labelsize\": 9,     \"legend.fontsize\": 9,     \"figure.titlesize\": 13 })  # Fun\u00e7\u00f5es auxiliares de m\u00e9tricas def _to1d(a):     a = np.asarray(a, dtype=float)     return a.ravel() if a.ndim &gt; 1 else a  def mape(y_true, y_pred, eps=1e-8):     y_true = np.asarray(y_true, dtype=float)     denom = np.maximum(np.abs(y_true), eps)     return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0  def smape(y_true, y_pred, eps=1e-8):     y_true = np.asarray(y_true, dtype=float)     y_pred = np.asarray(y_pred, dtype=float)     denom = np.maximum((np.abs(y_true) + np.abs(y_pred)) / 2, eps)     return np.mean(np.abs(y_true - y_pred) / denom) * 100.0  def r2_score_np(y_true, y_pred):     y_true = np.asarray(y_true, dtype=float)     ss_res = np.sum((y_true - y_pred)**2)     ss_tot = np.sum((y_true - np.mean(y_true))**2)     return 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan  # Dados e previs\u00f5es X_val   = np.asarray(X_val, dtype=float) X_test  = np.asarray(X_test, dtype=float) y_train = _to1d(y_train) y_val   = _to1d(y_val) y_test  = _to1d(y_test)  y_pred_val  = _to1d(predict(X_val)) y_pred_test = _to1d(predict(X_test))  baseline_val  = np.full_like(y_val,  fill_value=float(np.mean(y_train)), dtype=float) baseline_test = np.full_like(y_test, fill_value=float(np.mean(y_train)), dtype=float)  # Fun\u00e7\u00e3o de m\u00e9tricas gerais def print_metrics(title, y_true, y_pred, y_base):     mae  = np.mean(np.abs(y_true - y_pred))     mse  = np.mean((y_true - y_pred)**2)     rmse = sqrt(mse)     r2   = r2_score_np(y_true, y_pred)     mp   = mape(y_true, y_pred)     smp  = smape(y_true, y_pred)      mae_b  = np.mean(np.abs(y_true - y_base))     mse_b  = np.mean((y_true - y_base)**2)     rmse_b = sqrt(mse_b)     r2_b   = r2_score_np(y_true, y_base)     mp_b   = mape(y_true, y_base)     smp_b  = smape(y_true, y_base)      print(f\"\\n=== {title} ===\")     print(\"Metric           Model         Baseline(mean y_train)\")     print(f\"MAE      :   {mae:10.4f}   {mae_b:10.4f}\")     print(f\"MSE      :   {mse:10.4f}   {mse_b:10.4f}\")     print(f\"RMSE     :   {rmse:10.4f}   {rmse_b:10.4f}\")     print(f\"R2       :   {r2:10.4f}   {r2_b:10.4f}\")     print(f\"MAPE(%)  :   {mp:10.4f}   {mp_b:10.4f}\")     print(f\"sMAPE(%) :   {smp:10.4f}   {smp_b:10.4f}\")  print_metrics(\"Validation\", y_val,  y_pred_val,  baseline_val) print_metrics(\"Test\",       y_test, y_pred_test, baseline_test)  # ------------------------------------------------------------ train_acc = 1 - np.array(train_losses) / np.max(train_losses) val_acc   = 1 - np.array(val_losses) / np.max(val_losses)  ss_tot_val = np.sum((y_val - np.mean(y_val))**2) r2_by_epoch = [1 - (mse * len(y_val)) / ss_tot_val for mse in val_losses]  final_mse_test = np.mean((y_pred_test - y_test)**2) test_acc = 1 - final_mse_test / np.max(train_losses)  plt.figure(figsize=(15,4))  plt.subplot(1,3,1) plt.plot(train_losses, label=\"Train Loss\") plt.plot(val_losses, label=\"Val Loss\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\") plt.title(\"Curva de Loss por \u00c9poca\") plt.legend()  plt.subplot(1,3,2) plt.plot(train_acc, label=\"Train Accuracy (1 - Loss Normalizada)\") plt.plot(val_acc, label=\"Val Accuracy (1 - Loss Normalizada)\") plt.hlines(test_acc, 0, len(train_acc)-1, colors='orange', linestyles='--', label=f\"Test Accuracy ({test_acc:.3f})\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"Acur\u00e1cia (Normalizada)\") plt.title(\"Acur\u00e1cia por \u00c9poca (Train / Val / Test)\") plt.legend()  plt.subplot(1,3,3) plt.plot(r2_by_epoch, color='green', label=\"R\u00b2 (Val)\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"R\u00b2\") plt.title(\"R\u00b2 por \u00c9poca (Valida\u00e7\u00e3o)\") plt.ylim(0, 1.05) plt.legend()  plt.tight_layout() plt.show()  # 2) Gr\u00e1ficos de res\u00edduos e previs\u00e3o x real res_val = y_val - y_pred_val plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.scatter(y_pred_val, res_val, alpha=0.6) plt.axhline(0, linestyle=\"--\") plt.xlabel(\"Predicted (val)\"); plt.ylabel(\"Residuals\") plt.title(\"Residual Plot (Val)\")  plt.subplot(1,2,2) plt.scatter(y_val, y_pred_val, alpha=0.6) mn = float(min(np.min(y_val), np.min(y_pred_val))) mx = float(max(np.max(y_val), np.max(y_pred_val))) plt.plot([mn, mx], [mn, mx], linestyle=\"--\") plt.xlabel(\"Actual (y_val)\"); plt.ylabel(\"Predicted (val)\") plt.title(\"Actual vs Predicted (Val)\") plt.tight_layout() plt.show()  res_test = y_test - y_pred_test plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.scatter(y_pred_test, res_test, alpha=0.6) plt.axhline(0, linestyle=\"--\") plt.xlabel(\"Predicted (test)\"); plt.ylabel(\"Residuals\") plt.title(\"Residual Plot (Test)\")  plt.subplot(1,2,2) plt.scatter(y_test, y_pred_test, alpha=0.6) mn = float(min(np.min(y_test), np.min(y_pred_test))) mx = float(max(np.max(y_test), np.max(y_pred_test))) plt.plot([mn, mx], [mn, mx], linestyle=\"--\") plt.xlabel(\"Actual (y_test)\"); plt.ylabel(\"Predicted (test)\") plt.title(\"Actual vs Predicted (Test)\") plt.tight_layout() plt.show()  <pre>=== Validation ===\nMetric           Model         Baseline(mean y_train)\nMAE      :    2719.2841   19825.9131\nMSE      :   21706152.6864   519108303.5968\nRMSE     :    4658.9862   22783.9484\nR2       :       0.9582      -0.0000\nMAPE(%)  :      20.3035     238.0368\nsMAPE(%) :      18.8756     101.0205\n\n=== Test ===\nMetric           Model         Baseline(mean y_train)\nMAE      :    2702.1777   19702.5628\nMSE      :   20740489.7360   510225579.3527\nRMSE     :    4554.1728   22588.1734\nR2       :       0.9593      -0.0000\nMAPE(%)  :      20.5237     239.3193\nsMAPE(%) :      19.0806     100.9381\n</pre>"},{"location":"#ann-regression","title":"ANN Regression\u00b6","text":"<p>Autores: Lucas Lima, Henrique Badin e Eduardo Selber.</p> <p>Dataset Escolhido: Fligt Price Prediction.</p> <p>Notebook De Refer\u00eancia: Flight Price Prediction | EDA | Regression &amp; ANN</p>"},{"location":"#descricao-do-conjunto-de-dados","title":"Descri\u00e7\u00e3o do Conjunto de Dados\u00b6","text":"<p>O conjunto de dados cont\u00e9m informa\u00e7\u00f5es sobre passagens a\u00e9reas obtidas do site EaseMyTrip, com o objetivo de analisar fatores que influenciam o pre\u00e7o das passagens e treinar um modelo de regress\u00e3o linear para previs\u00e3o. Foram coletados 300.261 registros entre 11 de fevereiro e 31 de mar\u00e7o de 2022, abrangendo voos entre as seis principais cidades da \u00cdndia.</p>"},{"location":"#estrutura-do-dataset","title":"Estrutura do Dataset\u00b6","text":"<p>O conjunto possui 11 vari\u00e1veis, sendo 10 preditoras e 1 alvo (Price):</p> Vari\u00e1vel Tipo Descri\u00e7\u00e3o Airline Categ\u00f3rica Companhia a\u00e9rea (6 valores \u00fanicos) Flight Categ\u00f3rica C\u00f3digo do voo Source City Categ\u00f3rica Cidade de origem Departure Time Categ\u00f3rica Faixa de hor\u00e1rio de partida Stops Categ\u00f3rica N\u00famero de escalas Arrival Time Categ\u00f3rica Faixa de hor\u00e1rio de chegada Destination City Categ\u00f3rica Cidade de destino Class Categ\u00f3rica Classe da passagem (Economy ou Business) Duration Num\u00e9rica cont\u00ednua Dura\u00e7\u00e3o do voo em horas Days Left Num\u00e9rica discreta Dias entre a compra e a data do voo Price Num\u00e9rica cont\u00ednua Vari\u00e1vel alvo \u2013 pre\u00e7o da passagem"},{"location":"#tipos-de-variaveis","title":"Tipos de Vari\u00e1veis\u00b6","text":"<ul> <li>Categ\u00f3ricas nominais: Airline, Flight, Source City, Destination City, Class</li> <li>Categ\u00f3ricas ordinais: Departure Time, Arrival Time, Stops</li> <li>Num\u00e9ricas: Duration, Days Left, Price</li> </ul>"},{"location":"#dataset-observation","title":"Dataset Observation\u00b6","text":""},{"location":"#analise-das-variaveis-categoricas","title":"An\u00e1lise das vari\u00e1veis categ\u00f3ricas\u00b6","text":""},{"location":"#analise-das-variaveis-numericas","title":"An\u00e1lise das vari\u00e1veis num\u00e9ricas\u00b6","text":""},{"location":"#analise-de-correlacao-da-variavel-target","title":"An\u00e1lise de correla\u00e7\u00e3o da vari\u00e1vel target\u00b6","text":""},{"location":"#pre-processamento","title":"Pr\u00e9 Processamento\u00b6","text":""},{"location":"#separacao-do-conjunto-de-testes-e-treino","title":"Separa\u00e7\u00e3o do conjunto de testes e treino\u00b6","text":""},{"location":"#normalizacao-das-variaveis","title":"Normaliza\u00e7\u00e3o das vari\u00e1veis\u00b6","text":""},{"location":"#rede-neural-artificial","title":"Rede Neural Artificial\u00b6","text":""},{"location":"#processo-de-treinamento","title":"Processo de Treinamento\u00b6","text":"<p>O treinamento ocorre via gradiente descendente estoc\u00e1stico com o otimizador Adam, uma varia\u00e7\u00e3o adaptativa que combina momentum e RMSProp para ajustar dinamicamente a taxa de aprendizado.</p> <p>Os principais par\u00e2metros adotados foram:</p> <ul> <li>Taxa de aprendizado (<code>lr</code>): 0.001</li> <li>Tamanho do batch (<code>batch size</code>): 128 amostras por atualiza\u00e7\u00e3o de gradiente</li> <li>N\u00famero de \u00e9pocas: 100</li> <li>Fun\u00e7\u00e3o de custo: Erro Quadr\u00e1tico M\u00e9dio (MSE), apropriada para regress\u00e3o.</li> </ul> <p>Durante o treinamento:</p> <ol> <li>O conjunto de treino \u00e9 embaralhado e dividido em batches.</li> <li>Para cada batch, \u00e9 feita a propaga\u00e7\u00e3o direta (forward pass) para gerar previs\u00f5es.</li> <li>O erro \u00e9 calculado via MSE.</li> <li>O erro \u00e9 propagado para tr\u00e1s (backpropagation) para calcular os gradientes das camadas.</li> <li>Os pesos e vieses s\u00e3o atualizados conforme as regras do Adam.</li> </ol> <p>Ao final de cada \u00e9poca, o c\u00f3digo tamb\u00e9m calcula a loss de valida\u00e7\u00e3o, permitindo monitorar a generaliza\u00e7\u00e3o do modelo ao longo do tempo.</p>"},{"location":"#avaliacao-e-resultados","title":"Avalia\u00e7\u00e3o e Resultados\u00b6","text":"<p>Ap\u00f3s o treinamento, o modelo foi avaliado nos conjuntos de valida\u00e7\u00e3o e teste, com as m\u00e9tricas:</p> <ul> <li>MAE (Mean Absolute Error)</li> <li>RMSE (Root Mean Squared Error)</li> <li>R\u00b2 (Coeficiente de Determina\u00e7\u00e3o)</li> </ul> <p>Os resultados obtidos mostram um modelo com erro m\u00e9dio relativamente baixo e alto coeficiente de determina\u00e7\u00e3o (R\u00b2 \u2248 0.96), indicando que a rede aprendeu bem as rela\u00e7\u00f5es entre as vari\u00e1veis de entrada e o pre\u00e7o.</p> <p>Al\u00e9m disso, foram gerados dois gr\u00e1ficos principais:</p> <ul> <li>Curva de loss (MSE) para treino e valida\u00e7\u00e3o, evidenciando a converg\u00eancia do modelo.</li> <li>Dispers\u00e3o entre valores reais e previstos (y_true vs y_pred), mostrando boa correla\u00e7\u00e3o e baixa dispers\u00e3o residual.</li> </ul>"},{"location":"#visualizcao-dos-resultados","title":"Visualiz\u00e7\u00e3o dos resultados\u00b6","text":""},{"location":"#conclusoes","title":"Conclus\u00f5es\u00b6","text":"<p>Foram analisadas as curvas de treinamento, as m\u00e9tricas de regress\u00e3o e os gr\u00e1ficos de desempenho do modelo de rede neural aplicado \u00e0 predi\u00e7\u00e3o do pre\u00e7o de passagens a\u00e9reas.</p> <p>Durante o treinamento, as curvas de loss (MSE) de treino e valida\u00e7\u00e3o apresentaram queda acentuada nas primeiras \u00e9pocas e estabiliza\u00e7\u00e3o progressiva a partir da 80\u00aa \u00e9poca, indicando converg\u00eancia e aus\u00eancia de overfitting. As curvas de treino e valida\u00e7\u00e3o mant\u00eam-se pr\u00f3ximas, o que evidencia boa generaliza\u00e7\u00e3o do modelo. O gr\u00e1fico de R\u00b2 por \u00e9poca confirma essa estabilidade, atingindo valores pr\u00f3ximos de 0,96, o que mostra que o modelo explica cerca de 96% da variabilidade dos pre\u00e7os.</p> <p>Os gr\u00e1ficos de res\u00edduos (Residual Plots) e Actual vs Predicted indicam uma forte correla\u00e7\u00e3o entre valores reais e previstos, com os pontos bem distribu\u00eddos ao longo da linha de refer\u00eancia. H\u00e1, contudo, ligeiro aumento da dispers\u00e3o para pre\u00e7os mais altos, o que sugere heterocedasticidade \u2014 isto \u00e9, erros ligeiramente maiores em passagens mais caras, um comportamento esperado em dados econ\u00f4micos.</p> <p>As m\u00e9tricas de regress\u00e3o confirmam a boa performance do modelo em rela\u00e7\u00e3o ao baseline (previs\u00e3o pela m\u00e9dia do treino):</p> M\u00e9trica Valida\u00e7\u00e3o (Modelo / Baseline) Teste (Modelo / Baseline) MAE 2.719 / 19.826 2.702 / 19.703 MSE 21.706.152 / 519.108.303 20.740.489 / 510.225.579 RMSE 4.659 / 22.783 4.554 / 22.588 R\u00b2 0.958 0.959 MAPE (%) 20.30 / 238.03 20.52 / 239.31 sMAPE (%) 18.88 / 101.02 19.08 / 100.94 <p>Os resultados mostram que a rede neural supera amplamente o baseline, apresentando erros absolutos e percentuais significativamente menores e um R\u00b2 elevado, o que evidencia um modelo robusto e bem ajustado.</p> <p>Em s\u00edntese:</p> <ul> <li>O modelo apresenta excelente capacidade preditiva e estabilidade, com erro m\u00e9dio (MAE) em torno de 2.700 unidades monet\u00e1rias e erro percentual pr\u00f3ximo de 20%.</li> <li>As curvas e o R\u00b2 mostram converg\u00eancia est\u00e1vel e boa generaliza\u00e7\u00e3o.</li> <li>Pequenos desvios para valores altos de pre\u00e7o indicam espa\u00e7o para aprimoramentos, como aplicar transforma\u00e7\u00f5es no alvo (<code>log(price)</code>) ou empregar fun\u00e7\u00f5es de perda mais robustas (MAE ou Huber).</li> <li>O desempenho geral demonstra que a arquitetura da rede foi adequada para capturar padr\u00f5es complexos no comportamento dos pre\u00e7os de passagens a\u00e9reas.</li> </ul> <p>OBS: Fizemos uma m\u00e9trica de acur\u00e1cia baseada no MSE, esta m\u00e9trica foi criada simplesmente pois na p\u00e1gina da disciplina, que descreve o projeto, pede um gr\u00e1fico de acur\u00e1cia por \u00e9poca.</p>"}]}